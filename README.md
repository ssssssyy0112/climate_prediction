# Climate data analysis and prediction project
## Desciption
This project is a course-based team project. In this project, we use a dataset from ECA&D and it contains decades to hundreds years of European climate data.
The detailed dataset description is  from shown from dataset_intro.md, and some more specific description can be found there.

## Project Structure
Below is generated by linux tool `tree`.

`dataset_intro.md` is a detailed description of our dataset.

`elastic` is the docker-compose yaml to deploy a 3-node elasticsearch clusters with 1-node kibana server.

`export.ndjson` is an exported mapping file, and you can import it in your kibana with previous data process steps to see those beautiful maps.

`pictures` includes pictures drawn by matplotlib.pyplot and screenshots from kibana mappings. The distribution features of the data are shown there.
Also, it contains several gif to demonstrate the time trend with a year or across several years.

`seaweedfs` is the docker-compose yaml to deploy a distributed file system `SeaweedFS`, master node, volume node and filer included.

`spark` directory contains not only the docker-compose yaml with its Dockerfile to construct a 1-master-4-worker spark cluster, it also contains our core logic codes in this project.
Those Python files are about data analysis and machine learning(prediction) using `PySpark`, `pandas` and other tools. Each Python file has a high-level description of its purpose in the beginning lines.

`sync_gif.py` is a helper Python file using `imageIO` to bring pngs into a gif. Our gif pictures are produced this way.

```
.
├── dataset_intro.md
├── elastic
│   └── docker-compose.yml
├── export.ndjson
├── pictures
│   ├── avg_temp
│   ├── cloud_19951001.png
│   ├── gif
│   ├── high_temp
│   ├── humidity
│   ├── humidity_19951001.png
│   ├── low_temp
│   ├── matplotlib_pic
│   ├── mix_19950401.png
│   ├── pressure
│   ├── rain
│   ├── sunshine
│   ├── wind
│   └── wind_speed
├── seaweedfs
│   └── docker-compose.yml
├── spark
│   ├── Dockerfile
│   ├── KMeans.py
│   ├── add_level.py
│   ├── aggregate_analysis.py
│   ├── country.py
│   ├── docker-compose.yml
│   ├── draw_analysis.py
│   ├── intermediate_data
│   ├── location.py
│   ├── migration.py
│   ├── pictures
│   ├── station_positions
│   ├── statistics.py
│   ├── training.py
│   └── write_data.py
└── sync_gif.py
```

## How to use?
Considering that our dataset is too large, the total size has exceeded the upper limit supported by Github GitLFS (1GB), 
we have placed the original dataset and Elasticsearch image on PKU disk. You can directly download the data file zip package on the network disk, 
run the pre-process codes in it, and then use above data processing and analysis code in `spark` directory of this project for later steps.

### Steps
1. Download data from ECA&D website. Unzip them, removing the not needed first 20 lines in each txt.
2. Deploy the SeaweedFS and EXPOSE port 8888, save those text files into SeaweedFS.
3. Deploy the Spark cluster.
4. Deploy the ElasticSearch cluster and EXPOSE port 9200. (If you also deployed kibana, EXPOSE port 5601)
5. Use code `spark/write_data.py` to fetch files from SeaweedFS, use spark to process them and write them into ElasticSearch.
6. Use code `country.py` and `location.py` to add geo_point fileds into ElasticSearch. Use other Python files to do statistics, train and visualize data.

Note: The urls of Filesystem, ElasticSearch might vary depending on your machine and settings. You need to modify them by yourself.


### Outer links
RawData fils  https://disk.pku.edu.cn:443/link/880E890CCF0D51599FFD307D503D4493    Validation: 2024-02-29 23:59

Next are a few tar files, which contain ready-to-use big data storage components. You only need to unzip it and run the binary in the `bin` directory to get the data we have already stored, ready to use.

Those two can run in my computer with `Linux {username} 5.15.133.1-microsoft-standard-WSL2 #1 SMP Thu Oct 5 21:02:42 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux`

ElasticSearch  https://disk.pku.edu.cn:443/link/318E4F28C506B9B6992C364DBF013768    Validation：2024-02-29 23:59

Kibana  https://disk.pku.edu.cn:443/link/2068BE598A9D627FFF295448200640BC     Validation：2024-02-29 23:59

After downloading ES and Kibana, unzip them, you will have two folders: `elasticsearch-8.11.1` and `kibana-8.11.1`.
Then you can use `./elasticsearch-8.11.1/bin/elasticsearch` and `./kibana-8.11.1` to run them, and jump to the Step 6 above to explore the data directly!

## Others
Before trying to run our projects, you'll need to have a basic knowledge about how to run apps with `docker-compose`.

Also, `kibana` has many features, and you may need to search its website for more usages.

